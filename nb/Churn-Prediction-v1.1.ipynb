{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "religious-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of notebook is compact and for directly generating features, modeling and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loved-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False # to enable auto complete\n",
    "import pandas as pd\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subsequent-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('Main_Churn_Perdictors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "configured-nevada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Main_Churn_Perdictors:Removing 100000 based on columns X which is\n",
      "                1.0 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Removing 100000 based on columns Y which is\n",
      "                1.0 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Total rows remove = 198958 which is \n",
      "            1.9900000000000002 % of the rows\n",
      "INFO:Main_Churn_Perdictors:clean fraction = 0.020299678340323406\n"
     ]
    }
   ],
   "source": [
    "# data cleaning\n",
    "# this function is designed to handle the long tailed distributions (heavily found in the data)\n",
    "# hence it makes sense to test it on heavil tailed distributions ()\n",
    "from numpy import random\n",
    "def remove_outliers_quantiles(df,columns,q=0.99):\n",
    "    # this function remove outliers based on quantiles, columns must be numeric\n",
    "    n_rows = df.shape[0]\n",
    "    running_idx = pd.Series(np.repeat(True,n_rows))\n",
    "    for col in columns:\n",
    "        if isinstance(df[col][0],(np.float_,np.int_)):\n",
    "            \n",
    "            val_q = np.quantile(a=df[col],q=q)\n",
    "            idx = df[col]<=val_q\n",
    "            running_idx = np.logical_and(running_idx,idx)\n",
    "\n",
    "            row_to_remove_count = idx.value_counts().sort_values().values[0]\n",
    "            logger.info(f\"\"\"Removing {row_to_remove_count} based on columns {col} which is\n",
    "                {np.round(1.0*row_to_remove_count/n_rows,4)*100} % of the rows\"\"\")\n",
    "        else:\n",
    "            logger.error(f'Cannot remove outliers from column {col} as it is not numeric!')\n",
    "        filterd_df = df.loc[running_idx,:]\n",
    "        \n",
    "    total_rows_removed = running_idx.value_counts().sort_values().values[0]\n",
    "    logger.info(f\"\"\"Total rows remove = {total_rows_removed} which is \n",
    "            {np.round(1.0*total_rows_removed/n_rows,4)*100} % of the rows\"\"\")\n",
    "    return filterd_df\n",
    "\n",
    "def test_remove_outliers_quantiles():\n",
    "    n = 10e6\n",
    "    alpha = 0.01\n",
    "    df = pd.DataFrame({'X':random.exponential(scale=0.1,size=int(n)),\\\n",
    "                       'Y': random.exponential(scale=0.01,size=int(n))})\n",
    "    \n",
    "    df_processed = remove_outliers_quantiles(df,['X','Y'],1-alpha)\n",
    "    n_processed = df_processed.shape[0]\n",
    "    fraction = (n-n_processed) / n_processed\n",
    "    # assert the right amount of data is cleaned\n",
    "    # note because the and condtion in filtering, the lower bound of filerting fraction is alpha\n",
    "    logger.info(f'clean fraction = {fraction}')\n",
    "    assert fraction >= alpha , 'cleaned less than exepcted fraction'\n",
    "test_remove_outliers_quantiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handy-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of Feature generation functions along with their unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "exotic-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(df,target_col):\n",
    "    if len(df[target_col].unique())!=2:\n",
    "        raise ValueError('Numer of target unique values must = 2')\n",
    "    count = df[target_col].value_counts(normalize=True).sort_values()\n",
    "    minority_df = df.loc[df[target_col]==count.index[0],:]\n",
    "    majority_df = df.loc[df[target_col]==count.index[1],:]\n",
    "    # upsampling\n",
    "    n_majority = majority_df.shape[0]\n",
    "    minority_df_upsampled = minority_df.sample(n=n_majority,replace=True)\n",
    "    balanced_df = majority_df.append(minority_df_upsampled,ignore_index = True)\n",
    "    count_balanced = balanced_df[target_col].value_counts(normalize=True)\n",
    "    return balanced_df\n",
    "def test_upsample():\n",
    "    logger = logging.getLogger('test_upsample')\n",
    "    n = 100\n",
    "    df = pd.DataFrame({'Y':np.repeat(0,100)})\n",
    "    df.loc[0:10,'Y'] = 1\n",
    "    #logger.info(f\"\"\"Y value counts = \\n{df['Y'].value_counts(normalize=True)}\"\"\")\n",
    "    df_upsampled = upsample(df,'Y') \n",
    "    #logger.info(f\"\"\"Y value counts = \\n{df_upsampled['Y'].value_counts(normalize=True)}\"\"\")\n",
    "    upsampled_counts = df_upsampled['Y'].value_counts(normalize=True)\n",
    "    assert np.abs(1-upsampled_counts.values[0]/upsampled_counts.values[1]) <= 1e-2\n",
    "    assert df_upsampled.shape[0] > df.shape[0]\n",
    "test_upsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "utility-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper to just get a scalar version of the mode\n",
    "# TODO , add simple unit test\n",
    "def get_mode_scalar(series):# wrapper to get scalar value, not a series\n",
    "    return pd.Series(series).mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "attached-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature generation function, focusing on recurring customers\n",
    "# intuitively, the less the better \n",
    "def avg_days_between_orders(dates,default = 365): # need to revisit the default value , set to be 1 year\n",
    "    # the main intuition is that the higher the number , the less loyal the customer\n",
    "    # for corner case of only 1 order, setting the default to high, which is in the same direction of less loyal\n",
    "    # customers\n",
    "    if len(dates) <2:\n",
    "        return default\n",
    "    dates_series = pd.Series(data=pd.to_datetime(dates).values).sort_values()\n",
    "    dates_series_lag1 = dates_series.shift(1)\n",
    "    time_delta =  dates_series - dates_series_lag1\n",
    "    time_delta_day = time_delta.apply(lambda x:x.days)\n",
    "    return np.nanmean(time_delta_day)\n",
    "def test_avg_days_between_orders():\n",
    "    dates = ['2020-01-20','2020-01-23','2020-01-30']\n",
    "    assert np.abs(avg_days_between_orders(dates)-5) <1e-3\n",
    "    dates = ['2020-01-20']\n",
    "    assert np.abs(avg_days_between_orders(dates)-365) <1e-3\n",
    "test_avg_days_between_orders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "operating-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since_last_order(dates,current_date):\n",
    "    dates = pd.to_datetime(dates)\n",
    "    max_date = np.nanmax(dates)\n",
    "    current_date = pd.to_datetime(current_date)\n",
    "    if current_date < max_date:\n",
    "        raise ValueError('Current date must be >= max date in the dateset')\n",
    "    return (current_date-max_date).days\n",
    "def test_time_since_last_order():\n",
    "    time_val = time_since_last_order(dates=['2020-01-20','2020-01-23','2020-01-30'],current_date='2020-02-02')\n",
    "    assert np.abs(time_val-3) <=1e-2\n",
    "test_time_since_last_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fleet-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix generation function\n",
    "def generate_feature_matrix(df,target,cache_filepath):\n",
    "    # TODO : improve the parameterization of the feature_marix function\n",
    "    logger = logging.getLogger('Feature Matrix Generator')\n",
    "    if cache_filepath is not None:\n",
    "        feature_mtx = pd.read_csv(cache_filepath)\n",
    "        logger.info(f'loading raw feature matrix from cache file {cache_filepath}')\n",
    "    else:\n",
    "        logger.info(f'Computing raw feature matrix from cache file {cache_filepath}')\n",
    "        df['order_date_2'] = df['order_date'] # a quick workaround to be able to generate 2 features from\n",
    "        # the same column at once\n",
    "        feature_mtx = df.groupby(by='customer_id',as_index = False)\\\n",
    "                .agg({'customer_order_rank':np.nanmax,'is_failed':np.nanmean,\\\n",
    "                      'delivery_fee':np.nanmean,'amount_paid':np.nanmean,\\\n",
    "                      'payment_id':get_mode_scalar,'platform_id':get_mode_scalar,\\\n",
    "                      'is_returning_customer':get_mode_scalar,\n",
    "                     'order_date':avg_days_between_orders,\\\n",
    "                      'order_date_2':lambda x:time_since_last_order(x,current_date=max_order_date)})\n",
    "        feature_mtx.to_csv(cache_filepath,index=False)\n",
    "    # remove outliers\n",
    "    logger.info(f'shape of raw feature matrix = {feature_mtx.shape[0]}')\n",
    "    feature_mtx = remove_outliers_quantiles(df=feature_mtx,columns=['delivery_fee', 'amount_paid'])\n",
    "    logger.info(f'shape of feature matrix after removing outliers= {feature_mtx.shape[0]}')\n",
    "    \n",
    "    # upsampling\n",
    "    feature_mtx = upsample(df=feature_mtx,target_col=target)\n",
    "    logger.info(f'shape of feature matrix after upsampling = {feature_mtx.shape[0]}')\n",
    "    \n",
    "    # one-hot encoding\n",
    "    feature_mtx = pd.get_dummies(data=feature_mtx,columns=['payment_id','platform_id'])\n",
    "    feature_mtx.drop(columns ='customer_id',inplace=True)\n",
    "    logger.info(f'shape of feature matrix after one-hot encoding = {feature_mtx.shape[0]}')\n",
    "    return feature_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "official-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Main:Data loaded and merged, with shape (786600, 14)\n",
      "INFO:Main:Generate feature matrix\n",
      "INFO:Feature Matrix Generator:loading raw feature matrix from cache file raw_feature_matrix.csv\n",
      "INFO:Feature Matrix Generator:shape of raw feature matrix = 245455\n",
      "INFO:Main_Churn_Perdictors:Removing 1058 based on columns delivery_fee which is\n",
      "                0.43 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Removing 2455 based on columns amount_paid which is\n",
      "                1.0 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Total rows remove = 3468 which is \n",
      "            1.41 % of the rows\n",
      "INFO:Feature Matrix Generator:shape of feature matrix after removing outliers= 241987\n",
      "INFO:Feature Matrix Generator:shape of feature matrix after upsampling = 373728\n",
      "INFO:Feature Matrix Generator:shape of feature matrix after one-hot encoding = 373728\n"
     ]
    }
   ],
   "source": [
    "# Main code snippet \n",
    "def main():\n",
    "    main_logger = logging.getLogger('Main')\n",
    "    # data loading and merging \n",
    "    orders_df = pd.read_csv('../data/machine_learning_challenge_order_data.csv')\n",
    "    labeled_df = pd.read_csv('../data/machine_learning_challenge_labeled_data.csv')\n",
    "    \n",
    "    merged_df = pd.merge(left=orders_df,right=labeled_df,on='customer_id')\n",
    "    # FIXME , to remove\n",
    "    main_logger.info(f'Data loaded and merged, with shape {merged_df.shape}')\n",
    "    #train_df_filtered = remove_outliers_quantiles()\n",
    "    main_logger.info('Generate feature matrix')\n",
    "    raw_feature_matrix_cache_file = 'raw_feature_matrix.csv'\n",
    "    target_col = 'is_returning_customer'\n",
    "    generate_feature_matrix(df=merged_df,target = target_col,cache_filepath=raw_feature_matrix_cache_file)\n",
    "    \n",
    "    # Model Buil\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
