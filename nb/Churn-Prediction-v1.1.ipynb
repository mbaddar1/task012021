{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "religious-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of notebook is compact and for directly generating features, modeling and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "current-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False # to enable auto complete\n",
    "import pandas as pd\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegressionCV,LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enclosed-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('Main_Churn_Perdictors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "latest-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Main_Churn_Perdictors:Removing 100000 based on columns X which is\n",
      "                1.0 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Removing 100000 based on columns Y which is\n",
      "                1.0 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Total rows remove = 199017 which is \n",
      "            1.9900000000000002 % of the rows\n",
      "INFO:Main_Churn_Perdictors:clean fraction = 0.020305820344755216\n"
     ]
    }
   ],
   "source": [
    "# data cleaning\n",
    "# this function is designed to handle the long tailed distributions (heavily found in the data)\n",
    "# hence it makes sense to test it on heavil tailed distributions ()\n",
    "from numpy import random\n",
    "def remove_outliers_quantiles(df,columns,q=0.99):\n",
    "    # this function remove outliers based on quantiles, columns must be numeric\n",
    "    n_rows = df.shape[0]\n",
    "    running_idx = pd.Series(np.repeat(True,n_rows))\n",
    "    for col in columns:\n",
    "        if isinstance(df[col][0],(np.float_,np.int_)):\n",
    "            \n",
    "            val_q = np.quantile(a=df[col],q=q)\n",
    "            idx = df[col]<=val_q\n",
    "            running_idx = np.logical_and(running_idx,idx)\n",
    "\n",
    "            row_to_remove_count = idx.value_counts().sort_values().values[0]\n",
    "            logger.info(f\"\"\"Removing {row_to_remove_count} based on columns {col} which is\n",
    "                {np.round(1.0*row_to_remove_count/n_rows,4)*100} % of the rows\"\"\")\n",
    "        else:\n",
    "            logger.error(f'Cannot remove outliers from column {col} as it is not numeric!')\n",
    "        filterd_df = df.loc[running_idx,:]\n",
    "        \n",
    "    total_rows_removed = running_idx.value_counts().sort_values().values[0]\n",
    "    logger.info(f\"\"\"Total rows remove = {total_rows_removed} which is \n",
    "            {np.round(1.0*total_rows_removed/n_rows,4)*100} % of the rows\"\"\")\n",
    "    return filterd_df\n",
    "\n",
    "def test_remove_outliers_quantiles():\n",
    "    n = 10e6\n",
    "    alpha = 0.01\n",
    "    df = pd.DataFrame({'X':random.exponential(scale=0.1,size=int(n)),\\\n",
    "                       'Y': random.exponential(scale=0.01,size=int(n))})\n",
    "    \n",
    "    df_processed = remove_outliers_quantiles(df,['X','Y'],1-alpha)\n",
    "    n_processed = df_processed.shape[0]\n",
    "    fraction = (n-n_processed) / n_processed\n",
    "    # assert the right amount of data is cleaned\n",
    "    # note because the and condtion in filtering, the lower bound of filerting fraction is alpha\n",
    "    logger.info(f'clean fraction = {fraction}')\n",
    "    assert fraction >= alpha , 'cleaned less than exepcted fraction'\n",
    "test_remove_outliers_quantiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handy-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of Feature generation functions along with their unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weird-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(df,target_col):\n",
    "    if len(df[target_col].unique())!=2:\n",
    "        raise ValueError('Numer of target unique values must = 2')\n",
    "    count = df[target_col].value_counts(normalize=True).sort_values()\n",
    "    minority_df = df.loc[df[target_col]==count.index[0],:]\n",
    "    majority_df = df.loc[df[target_col]==count.index[1],:]\n",
    "    # upsampling\n",
    "    n_majority = majority_df.shape[0]\n",
    "    minority_df_upsampled = minority_df.sample(n=n_majority,replace=True)\n",
    "    balanced_df = majority_df.append(minority_df_upsampled,ignore_index = True)\n",
    "    count_balanced = balanced_df[target_col].value_counts(normalize=True)\n",
    "    return balanced_df\n",
    "def test_upsample():\n",
    "    logger = logging.getLogger('test_upsample')\n",
    "    n = 100\n",
    "    df = pd.DataFrame({'Y':np.repeat(0,100)})\n",
    "    df.loc[0:10,'Y'] = 1\n",
    "    #logger.info(f\"\"\"Y value counts = \\n{df['Y'].value_counts(normalize=True)}\"\"\")\n",
    "    df_upsampled = upsample(df,'Y') \n",
    "    #logger.info(f\"\"\"Y value counts = \\n{df_upsampled['Y'].value_counts(normalize=True)}\"\"\")\n",
    "    upsampled_counts = df_upsampled['Y'].value_counts(normalize=True)\n",
    "    assert np.abs(1-upsampled_counts.values[0]/upsampled_counts.values[1]) <= 1e-2\n",
    "    assert df_upsampled.shape[0] > df.shape[0]\n",
    "test_upsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mature-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper to just get a scalar version of the mode\n",
    "# TODO , add simple unit test\n",
    "def get_mode_scalar(series):# wrapper to get scalar value, not a series\n",
    "    return pd.Series(series).mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "governmental-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature generation function, focusing on recurring customers\n",
    "# intuitively, the less the better \n",
    "def avg_days_between_orders(dates,default = 365): # need to revisit the default value , set to be 1 year\n",
    "    # the main intuition is that the higher the number , the less loyal the customer\n",
    "    # for corner case of only 1 order, setting the default to high, which is in the same direction of less loyal\n",
    "    # customers\n",
    "    if len(dates) <2:\n",
    "        return default\n",
    "    dates_series = pd.Series(data=pd.to_datetime(dates).values).sort_values()\n",
    "    dates_series_lag1 = dates_series.shift(1)\n",
    "    time_delta =  dates_series - dates_series_lag1\n",
    "    time_delta_day = time_delta.apply(lambda x:x.days)\n",
    "    return np.nanmean(time_delta_day)\n",
    "def test_avg_days_between_orders():\n",
    "    dates = ['2020-01-20','2020-01-23','2020-01-30']\n",
    "    assert np.abs(avg_days_between_orders(dates)-5) <1e-3\n",
    "    dates = ['2020-01-20']\n",
    "    assert np.abs(avg_days_between_orders(dates)-365) <1e-3\n",
    "test_avg_days_between_orders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unlimited-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since_last_order(dates,current_date):\n",
    "    dates = pd.to_datetime(dates)\n",
    "    max_date = np.nanmax(dates)\n",
    "    current_date = pd.to_datetime(current_date)\n",
    "    if current_date < max_date:\n",
    "        raise ValueError('Current date must be >= max date in the dateset')\n",
    "    return (current_date-max_date).days\n",
    "def test_time_since_last_order():\n",
    "    time_val = time_since_last_order(dates=['2020-01-20','2020-01-23','2020-01-30'],current_date='2020-02-02')\n",
    "    assert np.abs(time_val-3) <=1e-2\n",
    "test_time_since_last_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fleet-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix generation function\n",
    "def generate_feature_matrix(df,target,cache_filepath):\n",
    "    # TODO : improve the parameterization of the feature_marix function\n",
    "    logger = logging.getLogger('Feature Matrix Generator')\n",
    "    if cache_filepath is not None:\n",
    "        feature_mtx = pd.read_csv(cache_filepath)\n",
    "        logger.info(f'loading raw feature matrix from cache file {cache_filepath}')\n",
    "    else:\n",
    "        logger.info(f'Computing raw feature matrix from cache file {cache_filepath}')\n",
    "        df['order_date_2'] = df['order_date'] # a quick workaround to be able to generate 2 features from\n",
    "        # the same column at once\n",
    "        feature_mtx = df.groupby(by='customer_id',as_index = False)\\\n",
    "                .agg({'customer_order_rank':np.nanmax,'is_failed':np.nanmean,\\\n",
    "                      'delivery_fee':np.nanmean,'amount_paid':np.nanmean,\\\n",
    "                      'payment_id':get_mode_scalar,'platform_id':get_mode_scalar,\\\n",
    "                      'is_returning_customer':get_mode_scalar,\n",
    "                     'order_date':avg_days_between_orders,\\\n",
    "                      'order_date_2':lambda x:time_since_last_order(x,current_date=max_order_date)})\n",
    "        feature_mtx.to_csv(cache_filepath,index=False)\n",
    "    # remove outliers\n",
    "    logger.info(f'shape of raw feature matrix = {feature_mtx.shape[0]}')\n",
    "    feature_mtx = remove_outliers_quantiles(df=feature_mtx,columns=['delivery_fee', 'amount_paid'])\n",
    "    logger.info(f'shape of feature matrix after removing outliers= {feature_mtx.shape[0]}')\n",
    "    \n",
    "    # upsampling\n",
    "    feature_mtx = upsample(df=feature_mtx,target_col=target)\n",
    "    logger.info(f'shape of feature matrix after upsampling = {feature_mtx.shape[0]}')\n",
    "    \n",
    "    # one-hot encoding\n",
    "    feature_mtx = pd.get_dummies(data=feature_mtx,columns=['payment_id','platform_id'])\n",
    "    feature_mtx.drop(columns ='customer_id',inplace=True)\n",
    "    logger.info(f'shape of feature matrix after one-hot encoding = {feature_mtx.shape[0]}')\n",
    "    \n",
    "    # remove NaNs\n",
    "    # possible improvment is to impute via interpolation or simple prediction\n",
    "    # loss of data measure to be 0.4% in the v1.0 nb, so we can proceed with simple drop\n",
    "    feature_mtx.dropna(inplace=True)\n",
    "    logger.info(f'shape of feature matrix after na drop = {feature_mtx.shape[0]}')\n",
    "    return feature_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elementary-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_cv_estimator(X,y,params,estimator,scores,verbose):\n",
    "    # different C's to test how stronger regularization affect the final score\n",
    "    logger = logging.getLogger('GridCV')\n",
    "    best_scores = dict()\n",
    "    best_estimators = dict()\n",
    "    for score in scores:\n",
    "        logger.info(f'Using score {score}')\n",
    "        # k for K-fold CV should be configurable, set for 3 for simplicity\n",
    "        gs = GridSearchCV(estimator=estimator,param_grid=params, cv=3, scoring=score,verbose=verbose)\n",
    "        gs_fit = gs.fit(X=X,y=y)\n",
    "        logger.info(f'Best estimator = {gs_fit.best_estimator_}')\n",
    "        logger.info(f'best {score} = {gs_fit.best_score_}')\n",
    "        ret = dict()\n",
    "        # str casting for serializability \n",
    "        ret['best_scores'] = str(gs_fit.best_score_)\n",
    "        ret['best_estimators'] = str(gs_fit.best_estimator_)\n",
    "        \n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-paint",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Main:Data loaded and merged, with shape (786600, 14)\n",
      "INFO:Main:Generate feature matrix\n",
      "INFO:Feature Matrix Generator:loading raw feature matrix from cache file raw_feature_matrix.csv\n",
      "INFO:Feature Matrix Generator:shape of raw feature matrix = 245455\n",
      "INFO:Main_Churn_Perdictors:Removing 1058 based on columns delivery_fee which is\n",
      "                0.43 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Removing 2455 based on columns amount_paid which is\n",
      "                1.0 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Total rows remove = 3468 which is \n",
      "            1.41 % of the rows\n",
      "INFO:Feature Matrix Generator:shape of feature matrix after removing outliers= 241987\n",
      "INFO:Feature Matrix Generator:shape of feature matrix after upsampling = 373728\n",
      "INFO:Feature Matrix Generator:shape of feature matrix after one-hot encoding = 373728\n",
      "INFO:Feature Matrix Generator:shape of feature matrix after na drop = 372114\n",
      "INFO:Main_Churn_Perdictors:Building GridCV NN models\n",
      "INFO:GridCV:Using score roc_auc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV 1/3; 1/8] START activation=logistic, hidden_layer_sizes=32, solver=lbfgs....\n"
     ]
    }
   ],
   "source": [
    "# Main code snippet \n",
    "def main():\n",
    "    main_logger = logging.getLogger('Main')\n",
    "    # data loading and merging \n",
    "    orders_df = pd.read_csv('../data/machine_learning_challenge_order_data.csv')\n",
    "    labeled_df = pd.read_csv('../data/machine_learning_challenge_labeled_data.csv')\n",
    "    \n",
    "    merged_df = pd.merge(left=orders_df,right=labeled_df,on='customer_id')\n",
    "    # FIXME , to remove\n",
    "    main_logger.info(f'Data loaded and merged, with shape {merged_df.shape}')\n",
    "    #train_df_filtered = remove_outliers_quantiles()\n",
    "    main_logger.info('Generate feature matrix')\n",
    "    raw_feature_matrix_cache_file = 'raw_feature_matrix.csv'\n",
    "    target_col = 'is_returning_customer'\n",
    "    feature_mtx = generate_feature_matrix(df=merged_df,target = target_col\\\n",
    "                                          ,cache_filepath=raw_feature_matrix_cache_file)\n",
    "    \n",
    "    # Model Building\n",
    "    X_cols = ['customer_order_rank', 'is_failed', 'delivery_fee', 'amount_paid',\n",
    "        'avg_days_between_orders',\n",
    "       'time_since_last_order', 'payment_id_1491', 'payment_id_1523',\n",
    "       'payment_id_1619', 'payment_id_1779', 'payment_id_1811',\n",
    "       'platform_id_22167', 'platform_id_22263', 'platform_id_29463',\n",
    "       'platform_id_29495', 'platform_id_29751', 'platform_id_29815',\n",
    "       'platform_id_30135', 'platform_id_30199', 'platform_id_30231',\n",
    "       'platform_id_30359', 'platform_id_30391', 'platform_id_30423',\n",
    "       'platform_id_525']\n",
    "    X = feature_mtx.loc[:,X_cols]\n",
    "    y = feature_mtx.loc[:,target_col]\n",
    "    \n",
    "    # This code snippet can be optimized further by a config file and a generic for loop\n",
    "    # bet left like that for simplicity and to control which models to run\n",
    "    \n",
    "    # use this dictionary switches to decide which model to run\n",
    "    \n",
    "    model_running_switch = {'lr':False,'ada':False,'nn':True}\n",
    "    if model_running_switch['lr']:\n",
    "    # Model 1 : Logisitc Regression\n",
    "        lr_params={'C':[0.1,1,10]} # different C's to test how stronger regularization affect the final score\n",
    "        scores = ['roc_auc','precision','recall']\n",
    "        logger.info(f'Building GridCV LR models')\n",
    "        verbose = 10\n",
    "        lr_return = grid_cv_estimator(X=X,y=y,estimator=LogisticRegression(),params=lr_params,\\\n",
    "                                      scores=scores,verbose = verbose)\n",
    "        logger.info('===================================================================')\n",
    "        logger.info(f'Best models for the logistic regression = {lr_return[0]}')\n",
    "        logger.info(f'Best score for the logistic regression = {lr_return[1]}')\n",
    "        logger.info('===================================================================')\n",
    "        with open(\"lr_performance.json\", \"w\") as fp:\n",
    "            json.dump(nn_return , fp)\n",
    "    \n",
    "    #################\n",
    "    \n",
    "    # Model 2 : Adaboost\n",
    "    if model_running_switch['ada']:\n",
    "        ada_parameters = {'n_estimators':[20,50,100],'learning_rate':[0.1,1,10]}\n",
    "        scores = ['roc_auc','precision','recall']\n",
    "        logger.info(f'Building GridCV AdaBoost models')\n",
    "        verbose = 10\n",
    "        ada_return = grid_cv_estimator(X=X,y=y,estimator=AdaBoostClassifier(),params=ada_parameters,\\\n",
    "                                      scores=scores,verbose = verbose)\n",
    "        logger.info('===================================================================')\n",
    "        logger.info(f'Best models for the adaboost = {ada_return[0]}')\n",
    "        logger.info(f'Best score for the adaboost = {ada_return[1]}')\n",
    "        logger.info('===================================================================')\n",
    "        with open(\"adaboost_performance.json\", \"w\") as fp:\n",
    "            json.dump(nn_return , fp)\n",
    "    \n",
    "    # Model 3 : Neural Nework\n",
    "    if model_running_switch['nn']:\n",
    "        nn_parameters = {'hidden_layer_sizes':[(32),(64,32)],'activation':['logistic','relu'],\\\n",
    "                         'solver':['lbfgs','adam']}\n",
    "        scores = ['roc_auc','precision','recall']\n",
    "        logger.info(f'Building GridCV NN models')\n",
    "        verbose = 10\n",
    "        # relu and logistic activation are suitable for classification problems\n",
    "        # lbfgs and adam are most commonly used in classification (lbfgs) is faster as it uses the Hessians,\n",
    "        # but a bit unstable\n",
    "        # experimenting 1 and 2 layer MLP\n",
    "        nn_return = grid_cv_estimator(X=X,y=y,estimator=MLPClassifier(),params=nn_parameters,\\\n",
    "                                      scores=scores,verbose = verbose)\n",
    "        logger.info('===================================================================')\n",
    "        logger.info(f\"\"\"Best models for the NN = {nn_return['best_estimators']}\"\"\")\n",
    "        logger.info(f\"\"\"Best score for the NN = {nn_return['best_scores']}\"\"\")\n",
    "        logger.info('===================================================================')\n",
    "        with open(\"nn_performance.json\", \"w\") as fp:\n",
    "            json.dump(nn_return , fp)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-works",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "1. Adaboost model gives better performance (slightly) over logistic regression\n",
    "2. Nonlineariy exists in feature vector target correlation, further non-linear models, like neural networks, can perform better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-client",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "1. apply customer segmentation to further understand customer behavior, for example , we can find 4 clusters based on \n",
    " frequency and paying level of customers, divided into high / low leverls\n",
    "2. apply NN to assign new customers to one of the segments\n",
    "3. experiment different models for each cusotmers , for example, adaboost model might perform the best in the\n",
    "    most frequent, highest paying customers, but logistic regression will perform the best in \n",
    "    low frequency low paying customers\n",
    "4. This 2 layer architecure despite being complex, but will be more interpretable to stakeholders and eventually \n",
    "  can lead to better results than \"one-size fits all\" model\n",
    "5. Apply better mothods to handle NA imputation instead of dropping, for example simple values prediction or interpolation, however I think this is a low priority since the NA drop fraction is very small, 0.4%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
