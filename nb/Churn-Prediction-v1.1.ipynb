{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "religious-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of notebook is compact and for directly generating features, modeling and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blind-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False # to enable auto complete\n",
    "import pandas as pd\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "growing-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('Main_Churn_Perdictors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "controversial-south",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Main_Churn_Perdictors:Removing 100000 based on columns X which is\n",
      "                1.0 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Removing 100000 based on columns Y which is\n",
      "                1.0 % of the rows\n",
      "INFO:Main_Churn_Perdictors:Total rows remove = 198958 which is \n",
      "            1.9900000000000002 % of the rows\n",
      "INFO:Main_Churn_Perdictors:clean fraction = 0.020299678340323406\n"
     ]
    }
   ],
   "source": [
    "# data cleaning\n",
    "# this function is designed to handle the long tailed distributions (heavily found in the data)\n",
    "# hence it makes sense to test it on heavil tailed distributions ()\n",
    "from numpy import random\n",
    "def remove_outliers_quantiles(df,columns,q=0.99):\n",
    "    # this function remove outliers based on quantiles, columns must be numeric\n",
    "    n_rows = df.shape[0]\n",
    "    running_idx = pd.Series(np.repeat(True,n_rows))\n",
    "    for col in columns:\n",
    "        if isinstance(df[col][0],(np.float_,np.int_)):\n",
    "            \n",
    "            val_q = np.quantile(a=df[col],q=q)\n",
    "            idx = df[col]<=val_q\n",
    "            running_idx = np.logical_and(running_idx,idx)\n",
    "\n",
    "            row_to_remove_count = idx.value_counts().sort_values().values[0]\n",
    "            logger.info(f\"\"\"Removing {row_to_remove_count} based on columns {col} which is\n",
    "                {np.round(1.0*row_to_remove_count/n_rows,4)*100} % of the rows\"\"\")\n",
    "        else:\n",
    "            logger.error(f'Cannot remove outliers from column {col} as it is not numeric!')\n",
    "        filterd_df = df.loc[running_idx,:]\n",
    "        \n",
    "    total_rows_removed = running_idx.value_counts().sort_values().values[0]\n",
    "    logger.info(f\"\"\"Total rows remove = {total_rows_removed} which is \n",
    "            {np.round(1.0*total_rows_removed/n_rows,4)*100} % of the rows\"\"\")\n",
    "    return filterd_df\n",
    "\n",
    "def test_remove_outliers_quantiles():\n",
    "    n = 10e6\n",
    "    alpha = 0.01\n",
    "    df = pd.DataFrame({'X':random.exponential(scale=0.1,size=int(n)),\\\n",
    "                       'Y': random.exponential(scale=0.01,size=int(n))})\n",
    "    \n",
    "    df_processed = remove_outliers_quantiles(df,['X','Y'],1-alpha)\n",
    "    n_processed = df_processed.shape[0]\n",
    "    fraction = (n-n_processed) / n_processed\n",
    "    # assert the right amount of data is cleaned\n",
    "    # note because the and condtion in filtering, the lower bound of filerting fraction is alpha\n",
    "    logger.info(f'clean fraction = {fraction}')\n",
    "    assert fraction >= alpha , 'cleaned less than exepcted fraction'\n",
    "test_remove_outliers_quantiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handy-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of Feature generation functions along with their unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "brutal-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(df,target_col):\n",
    "    if len(df[target_col].unique())!=2:\n",
    "        raise ValueError('Numer of target unique values must = 2')\n",
    "    count = df[target_col].value_counts(normalize=True).sort_values()\n",
    "    minority_df = df.loc[df[target_col]==count.index[0],:]\n",
    "    majority_df = df.loc[df[target_col]==count.index[1],:]\n",
    "    # upsampling\n",
    "    n_majority = majority_df.shape[0]\n",
    "    minority_df_upsampled = minority_df.sample(n=n_majority,replace=True)\n",
    "    balanced_df = majority_df.append(minority_df_upsampled,ignore_index = True)\n",
    "    count_balanced = balanced_df[target_col].value_counts(normalize=True)\n",
    "    return balanced_df\n",
    "def test_upsample():\n",
    "    logger = logging.getLogger('test_upsample')\n",
    "    n = 100\n",
    "    df = pd.DataFrame({'Y':np.repeat(0,100)})\n",
    "    df.loc[0:10,'Y'] = 1\n",
    "    #logger.info(f\"\"\"Y value counts = \\n{df['Y'].value_counts(normalize=True)}\"\"\")\n",
    "    df_upsampled = upsample(df,'Y') \n",
    "    #logger.info(f\"\"\"Y value counts = \\n{df_upsampled['Y'].value_counts(normalize=True)}\"\"\")\n",
    "    upsampled_counts = df_upsampled['Y'].value_counts(normalize=True)\n",
    "    assert np.abs(1-upsampled_counts.values[0]/upsampled_counts.values[1]) <= 1e-2\n",
    "    assert df_upsampled.shape[0] > df.shape[0]\n",
    "test_upsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fleet-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "narrow-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code snippet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "satisfied-plate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Main:Data loaded and merged, with shape (786600, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['customer_id', 'order_date', 'order_hour', 'customer_order_rank',\n",
      "       'is_failed', 'voucher_amount', 'delivery_fee', 'amount_paid',\n",
      "       'restaurant_id', 'city_id', 'payment_id', 'platform_id',\n",
      "       'transmission_id', 'is_returning_customer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    main_logger = logging.getLogger('Main')\n",
    "    # data loading and merging \n",
    "    orders_df = pd.read_csv('../data/machine_learning_challenge_order_data.csv')\n",
    "    labeled_df = pd.read_csv('../data/machine_learning_challenge_labeled_data.csv')\n",
    "    merged_df = pd.merge(left=orders_df,right=labeled_df,on='customer_id')\n",
    "    # FIXME , to remove\n",
    "    print(merged_df.columns)\n",
    "    main_logger.info(f'Data loaded and merged, with shape {merged_df.shape}')\n",
    "    #train_df_filtered = remove_outliers_quantiles()\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
